{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils \n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_trained = models.vgg16(pretrained=True)\n",
    "vgg16_untrained = models.vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def modify_model(model, input_channels, output_units):\n",
    "    '''\n",
    "    Parameters\n",
    "    \n",
    "    model: instance of a pytorch model to be modified\n",
    "    input_channels: channels of input tensor\n",
    "    output_units: number of units in the last layer\n",
    "    '''\n",
    "    model.features[0] = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "    model.classifier[6] = nn.Linear(4096, output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_model(vgg16_trained, 2, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with a random input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 2, 256, 256) # (256, 256, 3)\n",
    "output = vgg16_trained(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_trained.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_trained.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the untrained version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_model(vgg16_untrained, 2, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings = {\n",
    "#                 'criterion': nn.MSELoss,\n",
    "#                 'optimizer': optim.Adam,\n",
    "#                 'lr': 0.001\n",
    "#            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() # True if cuda is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model to cuda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if use_cuda:\n",
    "#     model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a simple model on Fashion MNIST dataset and setup tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.FashionMNIST(\n",
    "            root='./data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tensorboard setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "parameters = dict(\n",
    "    lr_list=[0.01, 0.001],\n",
    "    batch_size_list=[10, 128, 256, 512]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr=0.01, batch_size=10:\n",
      "\tEpoch 1, Training Loss: 39341.925781, Accuracy: 76.2967%\n",
      "\tEpoch 2, Training Loss: 32931.246094, Accuracy: 80.8117%\n",
      "\tEpoch 3, Training Loss: 33065.984375, Accuracy: 81.2100%\n",
      "\tEpoch 4, Training Loss: 31483.708984, Accuracy: 82.2083%\n",
      "\tEpoch 5, Training Loss: 32361.390625, Accuracy: 81.9967%\n",
      "\tEpoch 6, Training Loss: 33995.960938, Accuracy: 80.9950%\n",
      "\tEpoch 7, Training Loss: 34648.332031, Accuracy: 80.9800%\n",
      "\tEpoch 8, Training Loss: 32875.812500, Accuracy: 81.2833%\n",
      "\tEpoch 9, Training Loss: 32861.453125, Accuracy: 81.9000%\n",
      "\tEpoch 10, Training Loss: 32138.419922, Accuracy: 82.3583%\n",
      "\n",
      "lr=0.01, batch_size=128:\n",
      "\tEpoch 1, Training Loss: 32581.386719, Accuracy: 80.1950%\n",
      "\tEpoch 2, Training Loss: 23812.414062, Accuracy: 85.7583%\n",
      "\tEpoch 3, Training Loss: 22048.087891, Accuracy: 86.8033%\n",
      "\tEpoch 4, Training Loss: 21340.814453, Accuracy: 87.1100%\n",
      "\tEpoch 5, Training Loss: 20188.761719, Accuracy: 87.9833%\n",
      "\tEpoch 6, Training Loss: 19922.248047, Accuracy: 88.0533%\n",
      "\tEpoch 7, Training Loss: 19425.675781, Accuracy: 88.3100%\n",
      "\tEpoch 8, Training Loss: 19289.806641, Accuracy: 88.4117%\n",
      "\tEpoch 9, Training Loss: 18853.810547, Accuracy: 88.7733%\n",
      "\tEpoch 10, Training Loss: 18517.949219, Accuracy: 88.9150%\n",
      "\n",
      "lr=0.01, batch_size=256:\n",
      "\tEpoch 1, Training Loss: 36255.195312, Accuracy: 77.5250%\n",
      "\tEpoch 2, Training Loss: 23889.199219, Accuracy: 85.5667%\n",
      "\tEpoch 3, Training Loss: 21664.117188, Accuracy: 86.8567%\n",
      "\tEpoch 4, Training Loss: 20254.642578, Accuracy: 87.7250%\n",
      "\tEpoch 5, Training Loss: 19513.794922, Accuracy: 88.1633%\n",
      "\tEpoch 6, Training Loss: 18861.396484, Accuracy: 88.5667%\n",
      "\tEpoch 7, Training Loss: 17948.031250, Accuracy: 89.0150%\n",
      "\tEpoch 8, Training Loss: 17199.638672, Accuracy: 89.4233%\n",
      "\tEpoch 9, Training Loss: 17047.541016, Accuracy: 89.5817%\n",
      "\tEpoch 10, Training Loss: 17121.435547, Accuracy: 89.5200%\n",
      "\n",
      "lr=0.01, batch_size=512:\n",
      "\tEpoch 1, Training Loss: 39633.167969, Accuracy: 75.1217%\n",
      "\tEpoch 2, Training Loss: 23493.220703, Accuracy: 85.7667%\n",
      "\tEpoch 3, Training Loss: 21393.238281, Accuracy: 87.0917%\n",
      "\tEpoch 4, Training Loss: 19779.873047, Accuracy: 87.9417%\n",
      "\tEpoch 5, Training Loss: 18518.062500, Accuracy: 88.5883%\n",
      "\tEpoch 6, Training Loss: 17785.439453, Accuracy: 89.0650%\n",
      "\tEpoch 7, Training Loss: 17414.291016, Accuracy: 89.3250%\n",
      "\tEpoch 8, Training Loss: 16201.042969, Accuracy: 89.9433%\n",
      "\tEpoch 9, Training Loss: 16400.683594, Accuracy: 89.7717%\n",
      "\tEpoch 10, Training Loss: 16298.029297, Accuracy: 89.8633%\n",
      "\n",
      "lr=0.001, batch_size=10:\n",
      "\tEpoch 1, Training Loss: 30138.310547, Accuracy: 81.6450%\n",
      "\tEpoch 2, Training Loss: 22211.437500, Accuracy: 86.4850%\n",
      "\tEpoch 3, Training Loss: 20155.267578, Accuracy: 87.7767%\n",
      "\tEpoch 4, Training Loss: 18923.570312, Accuracy: 88.4367%\n",
      "\tEpoch 5, Training Loss: 17973.697266, Accuracy: 89.0033%\n",
      "\tEpoch 6, Training Loss: 17222.968750, Accuracy: 89.3950%\n",
      "\tEpoch 7, Training Loss: 16359.023438, Accuracy: 89.8500%\n",
      "\tEpoch 8, Training Loss: 15924.721680, Accuracy: 90.1000%\n",
      "\tEpoch 9, Training Loss: 15403.598633, Accuracy: 90.2967%\n",
      "\tEpoch 10, Training Loss: 15020.384766, Accuracy: 90.7583%\n",
      "\n",
      "lr=0.001, batch_size=128:\n",
      "\tEpoch 1, Training Loss: 37007.265625, Accuracy: 77.8517%\n",
      "\tEpoch 2, Training Loss: 24184.775391, Accuracy: 85.3817%\n",
      "\tEpoch 3, Training Loss: 21388.513672, Accuracy: 87.0217%\n",
      "\tEpoch 4, Training Loss: 19744.830078, Accuracy: 87.9233%\n",
      "\tEpoch 5, Training Loss: 18500.160156, Accuracy: 88.6117%\n",
      "\tEpoch 6, Training Loss: 17565.421875, Accuracy: 89.0767%\n",
      "\tEpoch 7, Training Loss: 16800.404297, Accuracy: 89.6050%\n",
      "\tEpoch 8, Training Loss: 15969.663086, Accuracy: 90.1367%\n",
      "\tEpoch 9, Training Loss: 15772.495117, Accuracy: 90.1900%\n",
      "\tEpoch 10, Training Loss: 14835.519531, Accuracy: 90.6683%\n",
      "\n",
      "lr=0.001, batch_size=256:\n",
      "\tEpoch 1, Training Loss: 43055.347656, Accuracy: 75.2333%\n",
      "\tEpoch 2, Training Loss: 26434.115234, Accuracy: 84.3317%\n",
      "\tEpoch 3, Training Loss: 23243.539062, Accuracy: 85.9483%\n",
      "\tEpoch 4, Training Loss: 21398.431641, Accuracy: 87.1050%\n",
      "\tEpoch 5, Training Loss: 19878.216797, Accuracy: 87.8467%\n",
      "\tEpoch 6, Training Loss: 18793.121094, Accuracy: 88.4850%\n",
      "\tEpoch 7, Training Loss: 17841.710938, Accuracy: 89.0383%\n",
      "\tEpoch 8, Training Loss: 17017.753906, Accuracy: 89.5400%\n",
      "\tEpoch 9, Training Loss: 16575.972656, Accuracy: 89.7750%\n",
      "\tEpoch 10, Training Loss: 15848.487305, Accuracy: 90.1817%\n",
      "\n",
      "lr=0.001, batch_size=512:\n",
      "\tEpoch 1, Training Loss: 50733.695312, Accuracy: 70.3350%\n",
      "\tEpoch 2, Training Loss: 28456.511719, Accuracy: 83.4733%\n",
      "\tEpoch 3, Training Loss: 25493.205078, Accuracy: 84.9783%\n",
      "\tEpoch 4, Training Loss: 23174.666016, Accuracy: 86.2133%\n",
      "\tEpoch 5, Training Loss: 21613.910156, Accuracy: 87.0883%\n",
      "\tEpoch 6, Training Loss: 20493.517578, Accuracy: 87.7983%\n",
      "\tEpoch 7, Training Loss: 19506.761719, Accuracy: 88.4250%\n",
      "\tEpoch 8, Training Loss: 18910.294922, Accuracy: 88.5933%\n",
      "\tEpoch 9, Training Loss: 18182.980469, Accuracy: 89.0150%\n",
      "\tEpoch 10, Training Loss: 17463.562500, Accuracy: 89.4083%\n"
     ]
    }
   ],
   "source": [
    "for (lr, batch_size) in it.product(*(parameter for parameter in parameters.values())):\n",
    "    \n",
    "    print('\\nlr={}, batch_size={}:'.format(lr, batch_size))\n",
    "    \n",
    "    # initialize model\n",
    "    model = Classifier().cuda() if use_cuda else Classifier()\n",
    "    \n",
    "    # create loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # initialize tensorboard\n",
    "    experiment = f' batch_size={batch_size} lr={lr}'\n",
    "    tb = SummaryWriter(comment=experiment) # tensorboard instance\n",
    "    images, labels = next(iter(train_loader))\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    grid = torchvision.utils.make_grid(images).cuda() # create a grid of images\n",
    "    \n",
    "    # add some graphs\n",
    "    tb.add_image('images', grid) # add batch of 100 images to tensorboard\n",
    "    tb.add_graph(model, images) # visualization of network inside tensorboard\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        # train \n",
    "        model.train() # keeps the gradients\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad() # reset gradients after each batch\n",
    "            output = model(data) # make prediction\n",
    "            loss = criterion(output, target) # calculate MSE\n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # update weights\n",
    "            train_loss += loss.data * batch_size # rescale back to compare different batch sizes\n",
    "            train_correct += get_num_correct(output, target)\n",
    "\n",
    "        accuracy = train_correct / len(train_set)\n",
    "        tb.add_scalar('Loss', train_loss, epoch)\n",
    "        tb.add_scalar('Number Correct', train_correct, epoch)\n",
    "        tb.add_scalar('Accuracy', accuracy, epoch)\n",
    "\n",
    "        tb.add_histogram('fc1.bias', model.fc1.bias, epoch)\n",
    "        tb.add_histogram('fc1.weight', model.fc1.weight, epoch)\n",
    "        tb.add_histogram(\n",
    "            'fc1.weight.grad'\n",
    "            ,model.fc1.weight.grad\n",
    "            ,epoch\n",
    "        )\n",
    "\n",
    "        print('\\tEpoch {}, Training Loss: {:.6f}, Accuracy: {:.4f}%'.format(epoch + 1, train_loss, accuracy * 100))\n",
    "\n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a train and test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    '''\n",
    "    Returns\n",
    "    \n",
    "    The trained model\n",
    "    '''\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.\n",
    "        valid_loss = 0.\n",
    "        \n",
    "        # train \n",
    "        model.train() # keeps the gradients\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            if use_cuda:\n",
    "                data, target = data.torch(), target.cuda()\n",
    "            optimizer = zero_grad() # reset gradients after each batch\n",
    "            output = model(data) # make prediction\n",
    "            loss = criterion(output, target) # calculate MSE\n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # update weights\n",
    "            mean_train_error = loss.data()\n",
    "            train_loss += ((1 / (batch_idx + 1))) * (loss.data() - train_loss)\n",
    "            \n",
    "        # validation\n",
    "        model.eval() # no need to keep grads\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data) # make prediction\n",
    "            loss = criterion(output, target) # calculate MSE\n",
    "            mean_valid_error = loss.data()\n",
    "            valid_loss += ((1 / (batch_idx + 1))) * (loss.data() - valid_loss)\n",
    "            \n",
    "        print('Epoch {}, Training Loss: {:.6f}, Mean Phase Error: {:.6f}, Validation Loss: {:.6f}, Mean Phase Error: {:.6f}'.format(train_loss, \n",
    "                                                                                                                                    mean_train_error,\n",
    "                                                                                                                                    valid_loss,\n",
    "                                                                                                                                    mean_valid_error))\n",
    "        print('Computation time: {:.4f} sec'.format(time.time() - start_time))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss has decreased from {:.6f} -> {:.6f}'.format(valid_loss_min, valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            torch.save(model_state_dict(), model)\n",
    "            \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
